{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SiamMOT-Capstone.ipynb","provenance":[],"collapsed_sections":["Xd2rRkF81KTo"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"683160be9afa4f109fa00cc313ec7138":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_09cc58457bca497baa3acef6a7096437","IPY_MODEL_e04f3b2d712e4ccfb032ab1d2ab11626","IPY_MODEL_3be427902cf14c8db87319a062babd7f"],"layout":"IPY_MODEL_b6a5211659d04cfe940b580eb6faec4e"}},"09cc58457bca497baa3acef6a7096437":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_70663d9691be499ea1b5f01262723dca","placeholder":"​","style":"IPY_MODEL_66717292cb4d43cb879c84862b3d1ff8","value":"100%"}},"e04f3b2d712e4ccfb032ab1d2ab11626":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dca8cf0186d54602bcd635ebacee2772","max":63228658,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a77d2ab6cd8e4b8eaef18d02005eb06e","value":63228658}},"3be427902cf14c8db87319a062babd7f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_973e65f681f446d68ab5abc2f22fb27f","placeholder":"​","style":"IPY_MODEL_f98f9f5083304a6d88731cfe2306e274","value":" 60.3M/60.3M [00:10&lt;00:00, 6.71MB/s]"}},"b6a5211659d04cfe940b580eb6faec4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70663d9691be499ea1b5f01262723dca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66717292cb4d43cb879c84862b3d1ff8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dca8cf0186d54602bcd635ebacee2772":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a77d2ab6cd8e4b8eaef18d02005eb06e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"973e65f681f446d68ab5abc2f22fb27f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f98f9f5083304a6d88731cfe2306e274":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# UE Computer Vision | Project : Visual tracking of video objects\n","\n","February 2022\n","\n","pierre-henri.conze@imt-atlantique.fr, ronan.fablet@imt-atlantique.fr\n","\n","aurelien.colin@imt-atlantique.fr, quentin.febvre@imt-atlantique.fr, matteo.zambra@imt-atlantique.fr"],"metadata":{"id":"1faAE5mZ_Bai"}},{"cell_type":"markdown","source":["## Purpose of the project\n","\n","Implement the methodology described in one of the two articles and apply it in the context of **visual tracking of video objects** within image sequences. Specifically, the targeted application consists in estimating for the whole sequence the location of a manually defined region of interest in the first image, called **reference image**. Development will be done in `python` programming language and can use functions from librairies such as `openCV`, `scikit-image`, `scikit-learn`, `keras`, `pytorch`... "],"metadata":{"id":"6pDerzcO_Fa0"}},{"cell_type":"markdown","source":["## Groupe 9 - Deep Learning\n","\n","**Authors :**\n","\n","\n","*   BERCY Victor\n","*   COSTE Paul\n","*   KALLALA Aymen\n","\n","\n","**Goal of the notebook :**\n","\n"],"metadata":{"id":"reNKcAb1_JiF"}},{"cell_type":"markdown","source":["## Install requirements"],"metadata":{"id":"bJ_TGoqHa2Ab"}},{"cell_type":"code","source":["from google.colab import drive\n","from os import chdir\n","\n","drive.mount(\"/content/drive\")\n","chdir(\"/content/drive/MyDrive/MCE/CompVi/Project/siam-mot-main\")"],"metadata":{"id":"SUJQygahWU9x","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648025346103,"user_tz":-60,"elapsed":25050,"user":{"displayName":"Victor B","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04317906044914918975"}},"outputId":"dc0ec8ba-d5c6-49c5-d046-e41ff6e257b2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!export CUDA_HOME=\"/usr/local/cuda-11.0\"\n","!export CUDA_PATH=\"$CUDA_HOME\"\n","!export PATH=\"$CUDA_HOME/bin:$PATH\"\n","!export LD_LIBRARY_PATH=\"$CUDA_HOME/lib64:$CUDA_HOME/lib:$CUDA_HOME/extras/CUPTI/lib64:$CUDA_HOME/efa/lib:$LD_LIBRARY_PATH\""],"metadata":{"id":"t_vUWP6_XVPx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install decord\n","!pip install ffmpeg-python\n","!pip install addict"],"metadata":{"id":"c95IcXma752l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!tar -xjvf ./x264-master.tar.bz2"],"metadata":{"id":"HDN8IYHqo14I"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OfUGDxE1WNur"},"outputs":[],"source":["!pip3 install torch==1.7.1+cu110 torchvision==0.8.2+cu110 -f https://download.pytorch.org/whl/torch_stable.html"]},{"cell_type":"code","source":["!pip3 install -r requirements.txt"],"metadata":{"id":"geKi263oWlZr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !pip uninstall datascience\n","# !pip uninstall albumentations"],"metadata":{"id":"zR1PikA4-sx9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chdir(\"/content/drive/MyDrive/MCE/CompVi/Project/siam-mot-main/x264-master/\")\n","!./configure --enable-shared --disable-asm"],"metadata":{"id":"D2VipDZHqcS_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!make"],"metadata":{"id":"e2e5oatoqxub"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!make install"],"metadata":{"id":"L91PhEqwq1kV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ldconfig"],"metadata":{"id":"ZMr5UAC5rM3c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/cocodataset/cocoapi.git\n","chdir(\"/content/drive/MyDrive/MCE/CompVi/Project/siam-mot-main/cocoapi/PythonAPI\")\n","!python setup.py build_ext install\n","chdir(\"/content/drive/MyDrive/MCE/CompVi/Project/siam-mot-main\")\n","!git clone https://github.com/NVIDIA/apex.git\n","chdir(\"/content/drive/MyDrive/MCE/CompVi/Project/siam-mot-main/apex\")\n","!python setup.py install --cuda_ext --cpp_ext\n","chdir(\"/content/drive/MyDrive/MCE/CompVi/Project/siam-mot-main\")\n","!git clone https://github.com/Idolized22/maskrcnn-benchmark.git\n","chdir(\"/content/drive/MyDrive/MCE/CompVi/Project/siam-mot-main/maskrcnn-benchmark\")\n","!python setup.py build develop\n","!find / -name 'miscellaneous.py'\n","!rm -rf /opt/conda/lib/python3.6/site-packages/maskrcnn_benchmark\n","!cp -r maskrcnn-benchmark/build/lib.linux-x86_64-3.6/maskrcnn_benchmark /opt/conda/lib/python3.6/site-packages/"],"metadata":{"id":"k8TZS9zqYGDk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cuda_dir=\"maskrcnn_benchmark/csrc/cuda\"\n","!perl -i -pe 's/AT_CHECK/TORCH_CHECK/' $cuda_dir/deform_pool_cuda.cu $cuda_dir/deform_conv_cuda.cu\n","# You can then run the regular setup command\n","!python3 setup.py build develop"],"metadata":{"id":"ZeWxruikXIPW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chdir(\"/content/drive/MyDrive/MCE/CompVi/Project/siam-mot-main\")"],"metadata":{"id":"clkuuZx5y-Zq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Import libraries"],"metadata":{"id":"oBQQUcG07mlV"}},{"cell_type":"code","source":["import os\n","import sys\n","import tqdm\n","import argparse\n","import pathlib\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import cv2\n","\n","import torch\n","from torchvision import transforms\n","\n","from skimage.exposure import rescale_intensity\n","import numpy as np\n","from skimage.segmentation import mark_boundaries\n","from skimage.exposure import rescale_intensity"],"metadata":{"id":"wq5w3rdH7iMy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"Xd2rRkF81KTo"}},{"cell_type":"code","source":["#To access the libraries downloaded\n","\n","sys.path.append('/content/drive/MyDrive/MCE/CompVi/Project/siam-mot-main/apex')\n","sys.path.append('/content/drive/MyDrive/MCE/CompVi/Project/siam-mot-main/maskrcnn-benchmark')"],"metadata":{"id":"CoSXUJIu5P-M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Informations about our dataset\n","\n","dataset_dir_path = '/content/drive/MyDrive/MCE/CompVi/Project/siam-mot-main/sequences-train-untouched/'\n","fps = 15\n","img_width, img_height = 360, 480\n","nbFrames = {\n","    'bag':      196, \n","    'bear':     26, \n","    'book':     51, \n","    'camel':    90, \n","    'rhino':    90, \n","    'swan':     50\n","}"],"metadata":{"id":"M46KqGut9TTO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_bbs(obj_name, path, nb_frames):\n","    \"\"\"\n","\n","    \"\"\"\n","    len_obj_name = len(obj_name)\n","    out = []\n","    os.chdir(path)\n","    out = [[0,0,0,0]] * nb_frames\n","\n","    for filename in os.listdir():\n","\n","        reg_mask = obj_name + '-[0-9]{3}.png'\n","\n","        if re.search(reg_mask,filename):\n","            img = cv2.imread(filename)\n","            x_min, x_max, y_min, y_max = get_bb_coos(img)\n","            index = int(filename[-7:-4]) # to match the # on the file name with the index in `out`\n","            out[index-1] = [x_min, x_max, y_min, y_max]\n","    \n","    return np.array(out)"],"metadata":{"id":"3FZ6ykaBJ-xM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_bb_coos(img):\n","    \"\"\"\n","    From a mask of a black and white pixels, outputs border bb coordinates in format (x_min, x_max, y_min, y_max)\n","    \"\"\"\n","    \n","    convert_tensor = transforms.ToTensor()\n","    img = convert_tensor(img)[0]\n","    nn0_img = np.nonzero(img)\n","    a,b = img.shape\n","    c = int(max(a,b))\n","    x_min, x_max, y_min, y_max = c, 0, c, 0\n","\n","    for x,y in nn0_img:\n","      x, y = int(x), int(y)\n","\n","      if x_min > x:\n","        x_min = x\n","      if x_max < x:\n","        x_max = x\n","      if y_min > y:\n","        y_min = y\n","      if y_max < y:\n","        y_max = y\n","\n","    return x_min, x_max, y_min, y_max\n","\n","def convert_chw_bb(x, y, height, width):\n","    \"\"\"\n","\n","    \"\"\"\n","\n","    x_min = x - width\n","    x_max = x + width\n","    y_min = y - height\n","    y_max = y + height\n","\n","    return np.array([x_min, x_max, y_min, y_max])\n","\n","def convert_corner_bb(bbox):\n","    \"\"\"\n","    Returns a bounding box of format (x_center, y_center, width, height) from format (x_min, x_max, y_min, y_max)\n","    \"\"\"\n","\n","    x_center, y_center = 1/2*(bbox[0]+bbox[1]), 1/2*(bbox[2]+bbox[3])\n","    width = bbox[1] - bbox[0]\n","    height = bbox[3] - bbox[2]\n","\n","    return [x_center, y_center, height/2, width/2]"],"metadata":{"id":"k796ov206RQJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ingestion script for our dataset\n","\n","from datetime import datetime\n","\n","from gluoncv.torch.data.gluoncv_motion_dataset.dataset import (\n","    AnnoEntity, DataSample, FieldNames, GluonCVMotionDataset, SplitNames\n",")\n","from gluoncv.torch.data.gluoncv_motion_dataset.utils.ingestion_utils import \\\n","    process_dataset_splits\n","\n","import gluoncv.torch.data.gluoncv_motion_dataset.dataset as gcvdataset\n","gcvdataset.__version__ = \"1.0\"\n","\n","_OBJECT_TYPES = ('bag', 'bear', 'book', 'camel', 'rhino', 'swan', 'others')\n","\n","_CLASS_LABELS = dict((vt, i) for i, vt in enumerate(_OBJECT_TYPES, start=1))\n","\n","def sample_from_raw(dataset_dir_path, fps, img_width, img_height, nbFrames):\n","\n","    seq_name = \"Train\"\n","    sample = DataSample(id=seq_name)\n","\n","    for object_type in _OBJECT_TYPES[:-1]:\n","        vehicle_type = object_type\n","        nb_frames = nbFrames[object_type]\n","        bboxes = get_bbs(object_type, dataset_dir_path + 'raw_data/', nb_frames)\n","\n","        for i, bbox in enumerate(bboxes):\n","            frame_num = i + 1\n","            frame_idx = i\n","\n","            obj_id = 1 #only one object per videos \n","            time_ms = int(round((frame_idx / fps) * 1000))\n","            entity = AnnoEntity(time=time_ms, id=obj_id)\n","            entity.confidence = 1.0\n","\n","            entity.bbox = convert_corner_bb(bbox)\n","\n","            entity.blob = {\n","                    'frame_xml':         frame_num,\n","                    'frame_idx':         frame_idx,\n","                    'color':             None, #attrib_attr['color'],\n","                    'orientation':       0., #float(attrib_attr['orientation']),\n","                    'speed':             1., #float(attrib_attr['speed']),\n","                    'trajectory_length': 10., #float(attrib_attr['trajectory_length']),\n","                    'truncation_ratio':  1., #float(attrib_attr['truncation_ratio']),\n","                    'vehicle_type':      vehicle_type,\n","                }\n","            entity.labels = {vehicle_type: _CLASS_LABELS[vehicle_type]}\n","            \n","            region_overlap = None #target.find('.//region_overlap')\n","            if region_overlap is not None:\n","                region_overlap_attr = region_overlap.attrib\n","                occlusion_status = region_overlap_attr['occlusion_status']\n","                occlusion_box = convert_corner_bb(region_overlap_attr)\n","                entity.blob['occlusion_status'] = int(occlusion_status)\n","                entity.blob['occlusion_box'] = occlusion_box\n","\n","            sample.add_entity(entity)\n","    \n","    # Need to replace the Windows path separator by UNIX-like to make the path\n","    # working across different platforms. Linux struggles with mixing path\n","    # separators whereas Windows does not.\n","    rel_data_path = os.path.join(dataset_dir_path, seq_name).replace('\\\\', '/')\n","    sample.metadata = {\n","        FieldNames.DATA_PATH:  rel_data_path,\n","        FieldNames.FPS:        fps,\n","        FieldNames.NUM_FRAMES: frame_num,\n","        FieldNames.RESOLUTION: {\n","            'width': img_width, 'height': img_height\n","        }\n","    }\n","\n","    return sample\n","\n","def ingest(dataset_dir_path, fps, img_width, img_height, nbFrames):\n","    dataset = GluonCVMotionDataset(\n","        annotation_file='anno.json', root_path=dataset_dir_path,\n","        load_anno=False\n","    )\n","    dataset.metadata = {\n","        FieldNames.DESCRIPTION:   \"IMT Atlantique Computer vision course dataset ingestion\",\n","        FieldNames.DATE_MODIFIED: str(datetime.now()),\n","    }\n","\n","    dataset_anno_dir = (\n","        pathlib.Path(dataset_dir_path) / GluonCVMotionDataset.DATA_DIR /\n","        'Covi_public'\n","    )\n","\n","    splits = ('_Train', dataset_anno_dir) #, ('Insight-MVT_Annotation_Test', dataset_anno_dir / '360p-Test'))\n","\n","    #tqdm_pbar = tqdm.tqdm(file=sys.stdout)\n","    #with tqdm_pbar as pbar:\n","        #for split_dir_name, split_dir in splits:\n","            #for sample_xml_file_path in map(str, split_dir.iterdir()):\n","                #pbar.set_description(f\"reading sample {split_dir}\")\n","    sample = sample_from_raw(\n","        dataset_dir_path, fps, img_width, img_height, nbFrames\n","    )\n","    dataset.add_sample(sample)\n","                #pbar.update()\n","    \n","    dataset.dump()\n","\n","    return dataset\n","\n","\n","def write_data_split(dataset):\n","    def split_func(sample):\n","        data_path = sample.data_relative_path\n","\n","        if 'Train' in data_path:\n","            return SplitNames.TRAIN\n","        elif 'Test' in data_path:\n","            return SplitNames.TEST\n","        \n","        raise RuntimeError(\"unrecognized data split\")\n","    \n","    process_dataset_splits(dataset, split_func, save=True)\n","\n","\n","def main_ingest_data(dataset_dir_path, fps, img_width, img_height, nbFrames):\n","    \n","    dataset = ingest(dataset_dir_path, fps, img_width, img_height, nbFrames)\n","    write_data_split(dataset)\n","\n","    return None"],"metadata":{"id":"kSwM5UDV4rix","executionInfo":{"status":"ok","timestamp":1648024793330,"user_tz":-60,"elapsed":2319,"user":{"displayName":"Paul C","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08123888862942850500"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"45580e5f-9865-45cb-a46e-ac958a3a4e16"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/gluoncv/__init__.py:40: UserWarning: Both `mxnet==1.9.0` and `torch==1.7.1+cu110` are installed. You might encounter increased GPU memory footprint if both framework are used at the same time.\n","  warnings.warn(f'Both `mxnet=={mx.__version__}` and `torch=={torch.__version__}` are installed. '\n"]}]},{"cell_type":"code","source":["import re\n","main_ingest_data(dataset_dir_path, fps, img_width, img_height, nbFrames)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hp41FktERtpB","executionInfo":{"status":"ok","timestamp":1648024943963,"user_tz":-60,"elapsed":71950,"user":{"displayName":"Paul C","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08123888862942850500"}},"outputId":"ab9b74d4-8f70-45ae-aa73-295cec625621"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:root:Skipping loading for annotation file /content/drive/MyDrive/MCE/CompVi/Project/siam-mot-main/sequences-train-untouched/annotation/anno.json\n","INFO:root:Split subpath: annotation/splits.json\n","/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:57: UserWarning: This overload of nonzero is deprecated:\n","\tnonzero()\n","Consider using one of the following signatures instead:\n","\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n","  return bound(*args, **kwds)\n"]},{"output_type":"stream","name":"stdout","text":["Processed 1 samples\n","Found 1 samples for train\n","Saving data splits to: /content/drive/MyDrive/MCE/CompVi/Project/siam-mot-main/sequences-train-untouched/annotation/splits.json\n"]}]},{"cell_type":"code","source":["chdir(\"/content/drive/MyDrive/MCE/CompVi/Project/siam-mot-main\")"],"metadata":{"id":"_H_mSQeEb_lk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python tools/train_net.py --config_file '/content/drive/MyDrive/MCE/CompVi/Project/siam-mot-main/siammot/configs/defaults.py' --train-dir '/content/drive/MyDrive/MCE/CompVi/Project/siam-mot-main/sequences-train-untouched/annotation'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AflSq-dE3a0w","executionInfo":{"status":"ok","timestamp":1648024840887,"user_tz":-60,"elapsed":5069,"user":{"displayName":"Paul C","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08123888862942850500"}},"outputId":"dbfa5806-5b0a-4149-de23-5c9243501496"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"tools/train_net.py\", line 12, in <module>\n","    from maskrcnn_benchmark.utils.miscellaneous import mkdir, save_config\n","ImportError: cannot import name 'save_config' from 'maskrcnn_benchmark.utils.miscellaneous' (/content/drive/.shortcut-targets-by-id/1nmICa9-r33D-HVcB3J8mLCKC1esIf7t3/Project/siam-mot-main/maskrcnn-benchmark/maskrcnn_benchmark/utils/miscellaneous.py)\n"]}]},{"cell_type":"markdown","source":["## Inference"],"metadata":{"id":"hkYDsSs9a7ET"}},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","from os import listdir\n","import re\n","\n","def save_video(name: str):\n","    \"\"\"\n","    Create a video in format .mp4 and save it in a folder with the name of the object studied\n","    /!\\ Colab : doesn't work for mysterious reasons...\n","    \"\"\"\n","\n","    reg = name + \"-[0-9]{3}.bmp\"\n","    path = \"./sequences-train-untouched/\"\n","    export_path = \"./sequences-train/\" + name + '/'\n","    print('source_path :', path)\n","    print('export_path :', export_path)\n","    \n","    img0 = cv2.imread(path + name + \"-001.bmp\")\n","    height, width, _ = img0.shape\n","    size = (width,height)\n","    out = cv2.VideoWriter(export_path + 'project-' + name + '.mp4', cv2.VideoWriter_fourcc(*'avc1'), fps, size)\n","\n","    for filename in listdir(path):\n","        if re.match(reg, filename):\n","            img = cv2.imread(path + filename)\n","            out.write(img)\n","\n","    out.release()\n","\n","    return None"],"metadata":{"id":"oTPQJlkAhPKl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["missing_videos = False #videos already saved\n","name_data = 'rhino' #Directory to work on\n","\n","if missing_videos:\n","    for data in NbFrames.keys():\n","        save_video(name_data)"],"metadata":{"id":"i1XGxQxVX9bD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from demos.demo_inference import DemoInference\n","from demos.utils import vis_writer as vw, vis_generator as vg\n","from demos.video_iterator import build_video_iterator\n","import os\n","\n","dump_video = True\n","vis_resolution = 360\n","\n","if step==\"train\":\n","    track_class = \"person_vehicle\"\n","    demo_video = \"sequences-train/Videos/input/project-\" + name_data + \".mp4\"\n","    output_path = \"sequences-train/Videos/output/\"\n","elif step==\"test\":\n","    track_class = \"person_vehicle\"\n","    demo_video = \"sequences-test/Videos/input/project-\" + name_data + \".mp4\"\n","    output_path = \"sequences-test/Videos/output/\"\n","\n","vis_generator = vg.VisGenerator(vis_height=vis_resolution)\n","vis_writer = vw.VisWriter(dump_video=dump_video,\n","                        out_path=output_path,\n","                        file_name=os.path.basename(demo_video))\n","    \n","# Build demo inference\n","tracker = DemoInference(track_class=track_class,\n","                        vis_generator=vis_generator,\n","                        vis_writer=vis_writer)\n","\n","# Build video iterator for inference\n","video_reader = build_video_iterator(demo_video)\n","\n","results = list(tracker.process_frame_sequence(video_reader()))\n","\n","if dump_video:\n","    vis_writer.close_video_writer()"],"metadata":{"id":"odpewh4E87FY","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["683160be9afa4f109fa00cc313ec7138","09cc58457bca497baa3acef6a7096437","e04f3b2d712e4ccfb032ab1d2ab11626","3be427902cf14c8db87319a062babd7f","b6a5211659d04cfe940b580eb6faec4e","70663d9691be499ea1b5f01262723dca","66717292cb4d43cb879c84862b3d1ff8","dca8cf0186d54602bcd635ebacee2772","a77d2ab6cd8e4b8eaef18d02005eb06e","973e65f681f446d68ab5abc2f22fb27f","f98f9f5083304a6d88731cfe2306e274"]},"executionInfo":{"status":"error","timestamp":1648025993867,"user_tz":-60,"elapsed":141928,"user":{"displayName":"Victor B","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04317906044914918975"}},"outputId":"bf497427-cfec-49a4-f7c2-b7acc1c4572f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"http://dl.yf.io/dla/models/imagenet/dla34-ba72cf86.pth\" to /root/.cache/torch/hub/checkpoints/dla34-ba72cf86.pth\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/60.3M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"683160be9afa4f109fa00cc313ec7138"}},"metadata":{}},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-ce1baf58f53a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m tracker = DemoInference(track_class=track_class,\n\u001b[1;32m     19\u001b[0m                         \u001b[0mvis_generator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvis_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                         vis_writer=vis_writer)\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Build video iterator for inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/MCE/CompVi/Project/siam-mot-main/demos/demo_inference.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, gpu_id, track_class, vis_generator, vis_writer)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_siam_augmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_and_load_tracker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/MCE/CompVi/Project/siam-mot-main/demos/demo_inference.py\u001b[0m in \u001b[0;36m_build_and_load_tracker\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_build_and_load_tracker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mtracker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_siammot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mtracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         checkpointer = DetectronCheckpointer(cfg, tracker,\n\u001b[1;32m     88\u001b[0m                                               save_dir=self.model_path)\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m     def register_backward_hook(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    608\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconvert_to_format\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"]}]},{"cell_type":"code","source":["from skimage.segmentation import mark_boundaries\n","#from skimage.exposure import rescale_intensity\n","\n","def visualize_bbox(img, bbox):\n","    \"\"\"\n","    Plot the bounding box of format [x_min, x_max, y_min, y_max] on the given image\n","    \"\"\"\n","\n","    #img = rescale_intensity(img, in_range=(np.min(img),np.max(img)), out_range=(0,1))\n","    xmin, xmax, ymin, ymax = int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])\n","\n","    mask = np.zeros(shape=img[:,:,0].shape, dtype=np.uint8)\n","    mask[xmin:xmax+1, ymin:ymax+1] = 1\n","    out = mark_boundaries(img, mask, color=(0, 255, 0))\n","\n","    return out\n","\n","def convert_chw_bb(bbox):\n","    \"\"\"\n","    Return a bounding box in format [x_min, x_max, y_min, y_max] from format [x_center, y_center, width, height]\n","    \"\"\"\n","\n","    x, y = bbox[0], bbox[1]\n","    width, height = bbox[2], bbox[3]\n","    \n","    x_min = x - width\n","    x_max = x + width\n","    y_min = y - height\n","    y_max = y + height\n","\n","    return np.array([x_min, x_max, y_min, y_max])"],"metadata":{"id":"_812_XcziFIT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","\n","path = '/content/drive/MyDrive/MCE/CompVi/Project/siam-mot-main/sequences-train/' + name_data + '/'\n","#outpath = '/content/drive/MyDrive/MCE/CompVi/Project/siam-mot-main/sequences-train/Videos/output/'\n","\n","#img0 = cv2.imread(path + name_data + \"-001.bmp\")\n","#height, width, _ = img0.shape\n","#size = (width, height)\n","#out = cv2.VideoWriter(outpath + 'output-' + name_data + '.avi', cv2.VideoWriter_fourcc(*'DVIX'), 15, size)\n","\n","mon_results = [] #for visualization purpose only\n","n = len(results)\n","\n","for i in range(n):\n","    num = str(i+1)\n","    end = '-' + (3-len(num))*'0' + num + '.bmp'\n","    img = cv2.imread(path + name_data + end)\n","    \n","    bboxes = results[i][1].bbox.numpy()\n","    bb = bboxes[0]\n","    bb = np.around(bb, decimals=0) #Round the coordinates to the nearest integer\n","    bb = convert_chw_bb(bb)\n","    mon_results.append(bb)\n","    img = visualize_bbox(img, bb)\n","    for bb in bboxes:\n","\n","        bb = np.around(bb, decimals=0) #Round the coordinates to the nearest integer\n","        bb = convert_chw_bb(bb)\n","        img = visualize_bbox(img, bb)\n","    \n","    plt.subplot(n, 1, i+1)\n","    plt.imshow(img)\n","\n","    #out.write(img)\n","\n","#out.release()"],"metadata":{"id":"dVZ5kghjGXP0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# results[2][1].bbox.numpy()"],"metadata":{"id":"xonYANr6Yir9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from os.path import isdir\n","\n","isdir(outpath)"],"metadata":{"id":"qAbcCsE4Vry5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# results[0][1]"],"metadata":{"id":"Nex5m6UHpe_W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# len(results[0][1])"],"metadata":{"id":"P-6tv4MgcXO3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.imshow(img, cmap='gray')"],"metadata":{"id":"VupE1LRlY0gM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","from maskrcnn_benchmark.utils.checkpoint import DetectronCheckpointer\n","from maskrcnn_benchmark.utils.miscellaneous import mkdir\n","\n","from siammot.configs.defaults import cfg\n","from siammot.modelling.rcnn import build_siammot\n","from siammot.engine.inferencer import DatasetInference\n","from siammot.utils.get_model_name import get_model_name\n","from siammot.data.adapters.utils.data_utils import load_dataset_anno, load_public_detection\n","from siammot.data.adapters.handler.data_filtering import build_data_filter_fn\n","\n","def test(cfg, test_dataset, output_dir, model_file=None, set=\"test\"):\n","\n","    torch.cuda.empty_cache()\n","\n","    # Construct model graph\n","    model = build_siammot(cfg)\n","    device = torch.device(cfg.MODEL.DEVICE)\n","    model.to(device)\n","\n","    # Load model params\n","    checkpointer = DetectronCheckpointer(cfg, model, save_dir=model_file)\n","    if os.path.isfile(model_file):\n","        _ = checkpointer.load(model_file)\n","    elif os.path.isdir(model_file):\n","        _ = checkpointer.load(use_latest=True)\n","    else:\n","        raise KeyError(\"No checkpoint is found\")\n","\n","    # Load testing dataset\n","    dataset_key = test_dataset\n","    dataset, modality = load_dataset_anno(cfg, dataset_key, set)\n","    dataset = sorted(dataset)\n","\n","    # do inference on dataset\n","    data_filter_fn = build_data_filter_fn(dataset_key)\n","\n","    # load public detection\n","    public_detection = None\n","    if cfg.INFERENCE.USE_GIVEN_DETECTIONS:\n","        public_detection = load_public_detection(cfg, dataset_key)\n","\n","    dataset_inference = DatasetInference(cfg, model, dataset, output_dir, data_filter_fn, public_detection)\n","    dataset_inference()\n","\n","\n","def main(output_dir, config_file, test_dataset=\"MOT17_DPM\", model_file=None):\n","    cfg.merge_from_file(config_file)\n","    cfg.freeze()\n","\n","    model_name = get_model_name(cfg)\n","    output_dir = os.path.join(output_dir, model_name)\n","    if not os.path.exists(output_dir):\n","        mkdir(output_dir)\n","\n","    test(cfg, output_dir, test_dataset, model_file, set)"],"metadata":{"id":"JHqd5OQPwewr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","# main(outpath, config_file)"],"metadata":{"id":"3idzxf67x_tj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluation"],"metadata":{"id":"A17SbCcAKpF2"}},{"cell_type":"code","source":["def metrics_centroid_distance(pred, GT):\n","    xmin, xmax, ymin, ymax = pred\n","    Xmin, Xmax, Ymin, Ymax = GT\n","    x,y, height, width = convert_corner_bb(xmin, xmax, ymin, ymax)\n","    X,Y, heightGT, widthGT = convert_corner_bb(Xmin, Xmax, Ymin, Ymax)\n","    return (abs(X-x)+abs(Y-y))/2"],"metadata":{"id":"9px4jSQwKr3f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def visualize_predicted_box(img, xmin, xmax, ymin, ymax):\n","    img = rescale_intensity(img, in_range=(np.min(img),np.max(img)), out_range=(0,1))\n","    mask = np.zeros(shape=img[:,:,0].shape, dtype=np.uint8)\n","    # mask = np.zeros(shape=img[:,:].shape, dtype=np.uint8)\n","    mask[xmin:xmax+1, ymin:ymax+1] = 1\n","    # print(mask[np.where(mask==1)])\n","    out = mark_boundaries(img, mask, color=(0, 255, 0))\n","    return out\n","    \n","def visualize_predicted_boxes(img, xmin, xmax, ymin, ymax, GTxmin, GTxmax, GTymin, GTymax):\n","    img = rescale_intensity(img, in_range=(np.min(img),np.max(img)), out_range=(0,1))\n","    mask = np.zeros(shape=img[:,:,0].shape, dtype=np.uint8)\n","    mask[xmin:xmax+1, ymin:ymax+1] = 1\n","    out = mark_boundaries(img, mask, color=(255, 0, 0), background_label=2)\n","    mask = np.zeros(shape=img[:,:,0].shape, dtype=np.uint8)\n","    mask[GTxmin:GTxmax+1, GTymin:GTymax+1] = 1\n","    out = mark_boundaries(out, mask, color=(0, 255, 0), background_label=4)\n","    return out"],"metadata":{"id":"pfRjb8hZAoKn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["directory = 'swan/'\n","path = '/content/drive/MyDrive/collab_2A_IMT/Project/siam-mot-main/sequences-train/'\n","# Get GT bbs\n","bb_swan = get_bbs('swan', path)\n","\n","path += directory\n","nbFrames = len(os.listdir(path))\n","print(nbFrames)"],"metadata":{"id":"Frl3kZA0FTVX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nbFrames=0 # to comment out whenn result is defined\n","met = [] #metrics values\n","\n","for i in range(nbFrames//2):\n","    plt.subplot(nbFrames, 1, i)\n","    plt.figure(figsize=(15,7))\n","    \n","    bb_GT = bb_swan[i]\n","    \n","    #read the i-th image\n","    num = str(i)\n","    num = (3-len(num))*'0' + num\n","    name = 'swan-' + num\n","    img = plt.imread(path + name + '.bmp')\n","    # img_mask = plt.imread(path + name + '.png') # not needed, computed before\n","    \n","    ###############################\n","    result = mon_results\n","    bb_pred = result # to be defined\n","    ###############################\n","    \n","    pred = result ### /!\\ format [x,x,y,y]\n","    GT = bb_GT\n","    curr_met = metrics_centroid_distance(pred, GT)\n","    met.append(curr_met)\n","\n","    xmin, xmax, ymin, ymax = pred\n","    Xmin, Xmax, Ymin, Ymax = GT\n","    vis_swan = visualize_predicted_boxes(img, xmin, xmax, ymin, ymax, Xmin, Xmax, Ymin, Ymax)\n","  \n","    plt.imshow(vis_swan)\n","    print('centroid distance :', curr_met)\n","    plt.axis('off')"],"metadata":{"id":"cXdfBA9OLvJy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(met)\n","plt.show()"],"metadata":{"id":"Ph8IlgecFx3l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# img = Image.open('/content/drive/MyDrive/MCE/CompVi/Project/siam-mot-main/sequences-train/swan/swan-001.bmp')\n","# img_mask = Image.open('/content/drive/MyDrive/MCE/CompVi/Project/siam-mot-main/sequences-train/swan/swan-001.png')\n","# bb_swan_1 = get_bb_coos(img_mask)\n","# xmin, xmax, ymin, ymax = bb_swan_1\n","# print(bb_swan_1)\n","# vis_swan_1 = visualize_predicted_box(img, xmin, xmax, ymin, ymax)\n","# vis2_swan_1 = visualize_predicted_boxes(img, xmin, xmax, ymin, ymax, xmin+1, xmax+1, ymin+1, ymax+1)\n","# # plt.imshow(vis_swan_1)\n","# plt.imshow(vis2_swan_1)\n","# plt.axis('off')\n","# plt.show()"],"metadata":{"id":"4E5aOVnSB9wa"},"execution_count":null,"outputs":[]}]}